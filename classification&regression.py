# -*- coding: utf-8 -*-
"""Classification&Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCqZ0w_xv7VNtTA9Uw9PR_gg_pQ332rd
"""

#Deciphering the type of iris flower
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt

iris = datasets.load_iris()
list(iris.keys())
X = iris["data"][:, 3:]#petal width
y = (iris["target"] == 2).astype(np.int) # 1 if Iris virginica, else 0

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X, y)

#flowers with petals with varied width between 0cm and 3 cm
X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)
plt.plot(X_new, y_proba[:, 1], "g-", label="Iris Virginica")
plt.plot(X_new, y_proba[:, 0], "b--", label="Not Iris Virginica")
plt.xlabel("Peta Width(cm)")
plt.ylabel("Probability")

#softmax regression
X = iris["data"][:, (2, 3)] #petal length, petal width
y = iris["target"]

softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)
softmax_reg.fit(X, y)
softmax_reg.predict([[5, 2]])
softmax_reg.predict_proba([[5, 2]])#length and width

#Support Vector Machine
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]# the target variable is a NumPy array containing the species of each iris flower (0 for Setosa, 1 for Versicolor, and 2 for Virginica
y = (iris["target"] == 2).astype(np.float64)

svm_clf = Pipeline([
    ("scaler", StandardScaler()),
    ("linear_svc", LinearSVC(C=1, loss="hinge")),
])

svm_clf.fit(X, y)
svm_clf.predict([[5.5, 1.7]])

#making nonlinear SVM classifiation more easy to predict by making it have curve-linear graph using polynomial features
from sklearn.datasets import make_moons
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVC

X, y = make_moons(n_samples=100, noise=0.15)
polynomial_svm_clf = Pipeline([
    ("poly_features", PolynomialFeatures(degree=3)),
    ("scaler", StandardScaler()),
    ("svm_clf", LinearSVC(C=10, loss="hinge"))
])
polynomial_svm_clf.fit(X, y)

svm_clf = SVC(kernel="rbf", C=10)#radial basis function
svm_clf.fit(X, y)

# Create a meshgrid for the decision boundary
h = 0.01 #mesh code for step size
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Obtain predictions on the meshgrid
Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the scatter plot
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o', s=80)
plt.contour(xx, yy, Z, colors='k', levels=[0], linestyles=['dashed'], linewidths=2)

# Set plot labels and show the plot
plt.title("Nonlinear SVM Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.close()

#using kernel trick to do the work of high degree polynomial without adding too many features like higher polynomials
poly_kernel_svm_clf = Pipeline([
    ("scaler", StandardScaler()),
    ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))#coef0 controls how much the model is influenced by highdegree polynomials versus low-degree polynomials
])
poly_kernel_clf.fit(X, y)

#using similarity features without adding so muxh features by using kernel
rbf_kernel_svm_clf = Pipeline([
    ("scaler", StandardScaler()),
    ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))
])
rbf_kernel_svm_clf.fit(X, y)

#SVM regression
from sklearn.svm import LinearSVR

svm_reg = LinearSVR(epsilon=1.5)
svm_reg.fit(X, y)

#for nonlinear SVM regression
from sklearn.svm import SVR

svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)
svm_poly_reg.fit(X, y)

