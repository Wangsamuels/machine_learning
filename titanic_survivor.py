# -*- coding: utf-8 -*-
"""Titanic_survivor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oM4Fus3QOrMBC4uMKvEnTDLLlG3GgEie
"""

import os
import urllib.request

TITANIC_PATH = os.path.join('datasets', 'titanic')
DOWNLOAD_URL = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/'

def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):
  if not os.path.isdir(path):
    os.makedirs(path)
  for filename in ('train.csv', 'test.csv'):
    filepath = os.path.join(path, filename)
    if not os.path.isfile(filepath):
      print('Downloading', filename)
      urllib.request.urlretrieve(url + filename, filepath)

fetch_titanic_data()

import pandas as pd

def load_titanic_data(filename, titanic_path=TITANIC_PATH):
  csv_path = os.path.join(titanic_path, filename)
  return pd.read_csv(csv_path)
train_data = load_titanic_data("train.csv")
test_data = load_titanic_data("test.csv")
train_data = train_data.set_index("PassengerId")
test_data = test_data.set_index("PassengerId")
train_data[train_data['Sex']=='female']['Age'].median()

train_data['Survived'].value_counts()

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy='median')),
    ("scaler", StandardScaler())
])

from sklearn.preprocessing import OneHotEncoder

cat_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("cat_encoder", OneHotEncoder(sparse=False)),
])

from sklearn.compose import ColumnTransformer

num_attribs = ["Age", "SibSp", "Parch", "Fare"]
cat_attribs = ["Pclass", "Sex", "Embarked"]

preprocess_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
])

X_train = preprocess_pipeline.fit_transform(
  train_data[num_attribs + cat_attribs])

y_train = train_data["Survived"]

from sklearn.ensemble import RandomForestClassifier

forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)
forest_clf.fit(X_train, y_train)

X_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])
y_pred = forest_clf.predict(X_test)

#to see how good the prediction is
from sklearn.model_selection import cross_val_score

forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)
print(forest_scores.mean())

from sklearn.svm import SVC

svm_clf = SVC(gamma='auto')
svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)
print(svm_scores.mean())

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))
plt.plot([1]*10, svm_scores, ".")
plt.plot([2]*10, forest_scores, ".")
plt.boxplot([svm_scores, forest_scores], labels=('SVM', 'Random Forest'))
plt.ylabel('Accuracy', fontsize=14)
plt.show()

#creating different groups by age, Siblings on board, parch, names and cabin
train_data["AgeBucket"] = train_data["Age"] // 15*15
print(train_data[["AgeBucket", "Survived"]].groupby(["AgeBucket"]).mean())

train_data["RelativesOnboard"] = train_data["SibSp"] + train_data["Parch"]
print(train_data[["RelativesOnboard", "Survived"]].groupby(["RelativesOnboard"]).mean())

# After obtaining the predictions
y_pred = forest_clf.predict(X_test)

# Create a DataFrame for the predictions
predictions_df = pd.DataFrame({
    "PassengerId": test_data.index,  # Assuming the index of test_data corresponds to PassengerId
    "Survived": y_pred
})

# Display the first few rows of the predictions DataFrame
print(predictions_df.head())

# Save this DataFrame to a CSV file
predictions_file_path = os.path.join(TITANIC_PATH, "predictions.csv")
predictions_df.to_csv(predictions_file_path, index=False)
print(f"Predictions saved to: {predictions_file_path}")







