# -*- coding: utf-8 -*-
"""Filter_spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12QfbFbqTaaGW2VEGSgkDwx8bpxmLkGkj
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import tarfile
import requests
from io import BytesIO

DOWNLOAD_ROOT = "http://spamassassin.apache.org/old/publiccorpus/"
HAM_URL = DOWNLOAD_ROOT + "20030228_hard_ham.tar.bz2"
SPAM_URL = DOWNLOAD_ROOT + "20030228_spam.tar.bz2"
SPAM_PATH = os.path.join("datasets", "spam")

def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):
    if not os.path.isdir(spam_path):
        os.makedirs(spam_path)
    for url in (ham_url, spam_url):
        response = requests.get(url)
        tar_bz2_file = tarfile.open(fileobj=BytesIO(response.content))
        tar_bz2_file.extractall(path=spam_path)
        tar_bz2_file.close()

fetch_spam_data()

HAM_DIR = os.path.join(SPAM_PATH, "hard_ham")
SPAM_DIR = os.path.join(SPAM_PATH, "spam")

ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]
spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]

len(ham_filenames)
len(spam_filenames)

import email, email.policy

def load_emails(is_spam, filename, spam_path=SPAM_PATH):
  directory = "spam" if is_spam else "hard_ham"
  with open(os.path.join(spam_path, directory, filename), "rb") as f:    #Opens the file in binary read mode ("rb")
    return email.parser.BytesParser(policy=email.policy.default).parse(f)

ham_emails = [load_emails(is_spam=False, filename=name) for name in ham_filenames]
spam_emails = [load_emails(is_spam=True, filename=name) for name in spam_filenames]

#taking sneak peek at the data for ham and spam
#print(ham_emails[1].get_content().strip())
#print(spam_emails[6].get_content().strip())

#since some emails have attachments and images. The different structures are observed
def get_email_structure(email):
  if isinstance(email, str):
    return email
  payload = email.get_payload()
  if isinstance(payload, list):
    return "multipart({})".format(", ".join([
        get_email_structure(sub_email)
        for sub_email in payload
    ]))
  else:
    return email.get_content_type()

#counts the number of occurence of a structure
from collections import Counter

def structures_counter(emails):
  structures = Counter()
  for email in emails:
    structure = get_email_structure(email)
    structures[structure] += 1
  return structures

structures_counter(ham_emails).most_common()
structures_counter(spam_emails).most_common()
#after accessing these two above, it shows that there's more plain text in ham_email and almost equal number of plain and html texts in spam_email

#for header, value in spam_emails[0].items():
  #print(header, ":", value)

spam_emails[0]["Subject"]

#splitting data into training and test sets
import numpy as np
from sklearn.model_selection import train_test_split

X = np.array(ham_emails + spam_emails, dtype=object)
y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#converting HTML to plain text
# The following function first drops the <head> section, then converts all <a> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the...
#...plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as &gt; or &nbsp;)
import re
from html import unescape
def html_to_plain_text(html):
  text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M |re.S| re.I)
  text = re.sub('<a\s.*?>', 'HYPERLINK', text, flags=re.M |re.S| re.I)
  text = re.sub('<.*?>', '', text, flags=re.M | re.S)
  text = re.sub(r'(\s*\n)+', '\n', text, flags=re.M | re.S)
  return unescape(text)

#to confirm if it works
html_spam_emails = [email for email in X_train[y_train==1] if get_email_structure(email) == 'text/html']
sample_html_spam = html_spam_emails[7]
#print(sample_html_spam.get_content().strip()[:1000], "...")
#print(html_to_plain_text(sample_html_spam.get_content())[:1000])

#taking in any email and returning it as plain text
def email_to_text(email):
    if isinstance(email, str):
        return email
    html = None
    for part in email.walk():
        ctype = part.get_content_type()
        if not ctype in ("text/plain", "text/html"):
            continue
        try:
            content = part.get_content()
        except:  # in case of encoding issues
            content = str(part.get_payload())
        if ctype == "text/plain":
            return content
        else:
            html = content
    if html:
        return html_to_plain_text(html)


#print(email_to_text(sample_html_spam)[:100], "...")

#aims to demonstrate word stemming using the Porter Stemmer algorithm from the NLTK (Natural Language Toolkit)
try:
  import nltk

  stemmer = nltk.PorterStemmer()
  for word in ("Computations", "Computation", "Computing", "Computed", "Compute", "Compulsive"):
    print(word, "=>", stemmer.stem(word))
except ImportError:
  print("Error: stemming requires the NLTK module.")
  stemmer = None

# %pip install -q -U urlextract

try:
  import urlextract
  url_extractor = urlextract.URLExtract()
  print(url_extractor.find_urls("Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s"))
except ImportError:
  print("Error: replacing URLs requires the urlextract module.")
  url_extractor = None

#preparing transformer that will convert the necessary things that has to be converted
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.sparse import csr_matrix

class EmailToWordCounterTransformer(BaseEstimator,  TransformerMixin):
  def __init__(self, strip_headers=True, lower_case=True, remove_punctuations=True, replace_urls=True, replace_numbers=True,
               stemming=True):
    self.strip_headers = strip_headers
    self.lower_case = lower_case
    self.remove_punctuations = remove_punctuations
    self.replace_urls = replace_urls
    self.replace_numbers = replace_numbers
    self.stemming = stemming
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    X_transformed = []
    for email in X:
      text = email_to_text(email) or ""
      if self.lower_case:
        text = text.lower()
      if self.replace_urls and url_extractor is not None:
        urls = list(set(url_extractor.find_urls(text)))
        urls.sort(key=lambda url: len(url), reverse=True)
        for url in urls:
          text = text.replace(url, "URL")
      if self.replace_numbers:
        text = re.sub(r'/d+(?:\.\d*)?(?:[eE][+-]?\d+)?', 'NUMBER', text)
      if self.remove_punctuations:
        text = re.sub(r'\W+', ' ', text, flags=re.M)
      words_count = Counter(text.split())
      if self.stemming and stemmer is not None:
        stemmed_word_counts = Counter()
        for word, count in words_count.items():
          stemmed_word = stemmer.stem(word)
          stemmed_word_counts[stemmed_word] += count
        words_counts = stemmed_word_counts
      X_transformed.append(words_counts)
    return np.array(X_transformed)

#  transforming emails
X_few = X_train[:3]
X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)
X_few_wordcounts

#converting the word counts to vector
from scipy.sparse import csr_matrix

class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):
  def __init__(self, vocabulary_size=1000):
    self.vocabulary_size = vocabulary_size
  def fit(self, X, y=None):
    total_count = Counter()
    for word_count in X:
      for word, count in word_count.items():
        total_count[word] += min(count, 10)
    most_common = total_count.most_common()[:self.vocabulary_size]
    self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}
    return self
  def transform(self, X, y=None):
    rows = []
    cols = []
    data = []
    for row, word_count in enumerate(X):
      for word, count in word_count.items():
        rows.append(row)
        cols.append(self.vocabulary_.get(word, 0))
        data.append(count)
    return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))

vocab_transformer =   WordCounterToVectorTransformer(vocabulary_size=10)
X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)
X_few_vectors.toarray()
#What does this matrix mean? Well, the 99 in the second row, first column, means that the second email contains 99 words that are not part of the vocabulary.
#....The 11 next to it means that the first word in the vocabulary is present 11 times in this email. The 9 next to it means that the second word is present 9 times,
#..... and so on. You can look at the vocabulary to know which words we are talking about. The first word is "the", the second word is "of", etc.
vocab_transformer.vocabulary_

#to train the whole dataset
from sklearn.pipeline import Pipeline

preprocess_pipeline = Pipeline([
    ("email_to_wordcount", EmailToWordCounterTransformer()),
    ("wordcount_to_vector", WordCounterToVectorTransformer())
])
X_train_transformed = preprocess_pipeline.fit_transform(X_train)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

log_clf = LogisticRegression(solver="lbfgs", max_iter=1000, random_state=42) #to be future-proof, we set solver="lbfgs" since this will be the default value in Scikit-Learn 0.22.
score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)
score.mean()

from sklearn.model_selection import GridSearchCV

# Define a range of hyperparameters to search
param_grid = {
    'C': [ 0.001, 0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear', 'sag', 'saga'],
    'max_iter': [100, 500, 1000]
}

# Create the GridSearchCV object
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=3, verbose=3)

# Perform the search on the hard_ham data
grid_search.fit(X_train_transformed, y_train)

# Get the best parameters
best_params = grid_search.best_params_
#print(best_params)

# Retrain the model using the best parameters
best_log_clf = grid_search.best_estimator_
score = cross_val_score(best_log_clf, X_train_transformed, y_train, cv=3, verbose=3)
score.mean()

from sklearn.metrics import precision_score, recall_score

X_test_transformed = preprocess_pipeline.transform(X_test)

log_clf = LogisticRegression(solver="lbfgs", max_iter=1000, random_state=42)
log_clf.fit(X_train_transformed, y_train)

y_pred = log_clf.predict(X_test_transformed)

#print("Precision: {:.2f}%".format(100 * precision_score(y_test, y_pred)))
#print("Recall: {:.2f}%".format(100 * recall_score(y_test, y_pred)))

#predicting whether an email is apm or ham
def predict_email(email, preprocess_pipeline, model):
    """
    Predicts if an email is spam or ham.

    Args:
    - email: Email content in plain text.
    - preprocess_pipeline: Pipeline used for preprocessing the data.
    - model: Trained classifier model.

    Returns:
    - prediction: 0 for ham and 1 for spam.
    """
    email_transformed = preprocess_pipeline.transform([email])
    prediction = model.predict(email_transformed)[0]
    return prediction
sample_email = input(f"Input the email content here: ")
prediction = predict_email(sample_email, preprocess_pipeline, log_clf)

if prediction == 0:
  print("Email is HAM.")
else:
  print("Email is SPAM.")



