# -*- coding: utf-8 -*-
"""Housing_price_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cJNCT9N8CdNwAtwU0DaLb5MM6MA0vPKP
"""

import os, numpy as np, joblib
from zlib import crc32
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import pandas as pd, matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder #used for converting texts to number- to be used for 'ocean_proximity'
from sklearn.preprocessing import OneHotEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from scipy import stats

HOUSING_PATH = "/content/housing.csv"

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path)
    return pd.read_csv(csv_path)

# Call the function to load housing data
housing = load_housing_data()
#print(housing['ocean_proximity'].value_counts())   counts numbers of ocean front houses by class
#print(housing.describe())   Summary of numerical attribute
#housing.hist(bins=50, figsize=(20,15), color='brown')    histogram of each data
#plt.show()

#splitting data set- using the idea of US population distribution noted in my note
def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]


train_set, test_set = split_train_test(housing, 0.2)
#print(len(train_set))

#print(len(test_set))

def test_set_check(identifier, test_ratio):
 return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
 ids = data[id_column]
 in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
 return data.loc[~in_test_set], data.loc[in_test_set]

housing_with_id = housing.reset_index() # adds an `index` column
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")

#since latitude and long would remain constant for many years so it could be combined into an 'ID'
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")

#Another way to split data is using sckit learn | second, multiple datasets with an identical number of rows and it will split them on same indices
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)# random_state is used to set random generator seed

#classifying a group by their income and getting an histogram chart
housing["income_cat"] = pd.cut(housing["median_income"],
 bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
 labels=[1, 2, 3, 4, 5])
#print(housing['median_income'].max())
housing["income_cat"].hist()
plt.xlabel('Income in 10^4 USD')
plt.ylabel('Number of people in the group')
#plt.show()

#data can be stratified based on income category
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
    strat_test_set['income_cat'].value_counts() / len(strat_test_set)
    #print(a)
    #print(b)
    #print(c)

#restoring the income_cat to it's ab initio state
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)

#scatter plot for the location of the houses in california
housing = strat_test_set.copy()
housing.plot(kind = 'scatter', x='longitude', y='latitude', alpha=0.1)#alpha is for better visualization of high desnsity \
#data points(it can be totally ignored) but, it's good for better visualization

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
 s=housing["population"]/100, label="population", figsize=(10,7),
 c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
)
#housing.plot(
#    kind="scatter",         # Type of plot: scatter plot
#    x="longitude",          # X-axis represents the longitude
 #   y="latitude",           # Y-axis represents the latitude
 #   alpha=0.4,               # Transparency of points (0.0 to 1.0)
 #   s=housing["population"]/100,  # Size of points, proportional to population.Larger populations will have larger points.
#    label="population",      # Label for the legend
 #   figsize=(10, 7),          # Size of the figure
 #   c="median_house_value",  # Color of points, based on median_house_value
#    cmap=plt.get_cmap("jet"),  # Color map (jet is a popular choice)
  #  colorbar=True            # Display color bar
#)
#result from the scatter plot = California housing prices: red is expensive, blue is cheap, larger circles indicate areas with a larger population

# Exclude the "ocean_proximity" column before calculating the correlation matrix
housing_no_ocean_proximity = housing.drop("ocean_proximity", axis=1)
corr_matrix = housing_no_ocean_proximity.corr()

# Display correlation with the target variable
correlation_with_target = corr_matrix["median_house_value"].sort_values(ascending=False)
#print(correlation_with_target)

#attributes = ["median_house_value", "median_income", "total_rooms",
 #"housing_median_age"]
#scatter_matrix(housing[attributes], figsize=(12, 8))
#housing.plot(kind="scatter", x="median_income", y="median_house_value",
 #alpha=0.1)

#to get each rooms and shii#
housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]
housing["population_per_household"]=housing["population"]/housing["households"]

# Include the new variables in the correlation matrix along with existing variables
all_variables = list(housing.columns)
existing_variables = [var for var in all_variables if var != "ocean_proximity"]
corr_matrix_with_new_variables = housing[existing_variables].corr()

# Display correlation with the target variable for all variables
correlation_with_all_variables = corr_matrix_with_new_variables["median_house_value"].sort_values(ascending=False)
#print(correlation_with_all_variables)

#addresses missing values in the "total_bedrooms" column by replacing them with the median
housing = strat_train_set.drop("median_house_value", axis=1) #note: axis o means row, axis 1 is column
housing_labels = strat_train_set["median_house_value"].copy()#copied from training set
median = housing["total_bedrooms"].median()
#print(housing['total_bedrooms'].fillna(median, inplace=True))

#used to handle missing values
imputer = SimpleImputer(strategy="median")
housing_num = housing.drop('ocean_proximity', axis=1)
#to fit the imputer instance to the training data
imputer.fit(housing_num)#you can use this “trained” imputer to transform the training set by replacing
#missing values with the learned medians
imputer.statistics_#median of each attribute is stored here
#print(a)
housing_num.median().values
#print(b)
X = imputer.transform(housing_num)
#print(X)#result is a Numpy array containing the transformed features

#to put it back to pandas Dataframe
housing_tr = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)
#print(housing_tr)
housing_cat = housing[['ocean_proximity']]
#print(housing_cat.head(10))

#to change the 'text' in ocean_proximity to numbers
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
#print(housing_cat_encoded[:10])

#print(ordinal_encoder.categories_)#list of 1D array of categories

#to categorize the categories whether it's present for each house or abscent for example: ocean proximity: If it's inland= 1, if not, =0
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
#print(housing_cat_1hot)

#instead of storing up zeros in the computer memory, instead a sparse matrix will store nonzero elements location
#print(housing_cat_1hot.toarray()[:10])

#print(cat_encoder.categories_)

# The purpose of this transformer is to add additional combined attributes to the input data to give more details about the x axis for the graph
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True):   #no *args or **kargs
      self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
       return self #nothing else to do
    def transform(self, X):   #X represent the features i.e: the x-axis
       rooms_per_household = X[:, rooms_ix] / X[:, households_ix] #used for selecting all the columns on the room array located on row 3
       population_per_household = X[:, population_ix] / X[:, rooms_ix]
       if self.add_bedrooms_per_room:
          bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
          return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]
       else:
          return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room = False)
housing_extra_attribs = attr_adder.transform(housing.values)
#print(housing_extra_attribs)

#sequencecial transformation using pipeline
num_pipeline = Pipeline([
   ('imputer', SimpleImputer(strategy='median')),
   ('attribs_adder', CombinedAttributesAdder()),
   ('std_scaler', StandardScaler())
])
housing_num_tr = num_pipeline.fit_transform(housing_num)

# Define numerical and categorical attributes
num_attribs = list(housing.select_dtypes(include=[np.number]))
cat_attribs = ['ocean_proximity']

#PLOTTING LINEAR REGRESSION GRAPH
# Create a ColumnTransformer that applies different transformers to numerical and categorical attributes
full_pipeline = ColumnTransformer([
    ("num", Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler())
    ]), num_attribs),
    ("cat", Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder())
    ]), cat_attribs)
])

# Apply the full pipeline to the entire dataset
housing_prepared = full_pipeline.fit_transform(housing)

# Train the linear regression model
lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

# Now, apply the same transformations to the test set
housing_test = strat_test_set.copy()
housing_test_prepared = full_pipeline.transform(housing_test)

some_labels = housing_labels.iloc[:5]

#print("Prediction:", lin_reg.predict(housing_test_prepared))
#print("Labels:", list(some_labels))

housing_predictions = lin_reg.predict(housing_prepared)
# The Mean Squared Error between the actual and predicted housing prices.
lin_mse = mean_squared_error(housing_labels, housing_predictions)
#to measure perfeormance of the regression model taking RMSE
lin_rmse = np.sqrt(lin_mse)
#print(lin_rmse) > gives the average price difference between house prices

#Used to find complex non-linear relationship in the data
tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)

#evaluating the trained set
housing_predictions = tree_reg.predict(housing_prepared)
#MSE between actual and predicted housing prices
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_mse = np.sqrt(tree_mse)
#print(tree_mse)  prints zero indicating that there's no error

#alternative to checking if there's error in the model
#the training_data_ is splitted into 10 places
scores = cross_val_score(tree_reg, housing_prepared, housing_labels, \
                         scoring = 'neg_mean_squared_error', cv=10)
tree_rmse_scores = np.sqrt(-scores)

#Another regression model... Second part is below
forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)

housing_prediction = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_prediction)
forest_rmse = np.sqrt(forest_mse)

#------> cross-validation test for whether decision tree is good for the model
#scikit-learn's cross validation features expects a utility function(bigger is better) than a cost function(smaller is better)
#  that is why the code computes '-scores'. This code computes the cost function
def display_scores(scores):
   print("Scores:", scores)
   print("Mean:", scores.mean())
   print("Standard Deviation:", scores.std())

#display_scores(tree_rmse_scores)#this is overfitting

#------> cross-validation test for whether linear regression is good for the model
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
#display_scores(lin_rmse_scores)
#Therefore, the decision tree is overfitting

#Another regression model that can be used is RandomForestRegressor- It works by training many decisio decision on random subsets of the feature,...
#then averaging them out.... The first part is above
forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)
#display_scores(forest_rmse_scores)#this still is overfitting

#to save hyperparameter and the trained parameters
#joblib.dump(my_model, 'my_model.pkl')
#my_model_loaded = joblib.load('my_model.pkl')

#fine tuning models
#searches for best hyperparameter values for the RandomForestRegressor to prevent overfitting of data
param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8],
     'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}
]


forest_reg = RandomForestRegressor()

grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error', return_train_score=True)
grid_search.fit(housing_prepared, housing_labels)
grid_search.best_params_#best parameter

grid_search.best_estimator_#best estimator
RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)

#the best evaluation scores are printed
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):
 # print(np.sqrt(-mean_score), params)
  #the grid search can be used to find out whether or not to add a feature you aren't sure about, even outliers, feature_selection and more
#for large hyperparameter search space, 'RandomSizedCV' class is used

#analyse best model and their errors
 feature_importances = grid_search.best_estimator_.feature_importances_
 feature_importances

#to display the importance next to their attribute name
extra_attribs = ['rooms_per_hhold',' pop_per_hhold', 'bedrooms_per_room']
cat_encoder = full_pipeline.named_transformers_['cat']['onehot']
cat_one_hot_attribs = list(cat_encoder.categories_[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
sorted(zip(feature_importances, attributes), reverse=True)

#to evaluate the system on the test set
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop('median_house_value', axis=1)
y_test = strat_test_set['median_house_value'].copy()

X_test_prepared = full_pipeline.transform(X_test)
final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)

#to know how precise the estimate is
confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc = squared_errors.mean(),
                         scale = stats.sem(squared_errors)))  #sem is standard error of mean
#plt.show

#plt.legend()

